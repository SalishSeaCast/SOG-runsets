{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read batch yaml\n",
    "with open('/ocean/bmoorema/research/SOG/sog-runsets/SOG_test.yaml', 'r') as f:\n",
    "    data = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "results_path = os.path.join(data['paths']['runs_directory'], data['batch_name'])\n",
    "initial_path = data['paths']['initial']\n",
    "ctd_dir = data['initialization']['ctd_directory']\n",
    "nuts_dir = data['initialization']['nutrients_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for forcing fields\n",
    "vary_forcings = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "if 'forcing1' in data:\n",
    "    if 'vary' in data['forcing']:\n",
    "        vary_forcings = data['forcing']['vary']\n",
    "\n",
    "# Check for river chemistry fields\n",
    "vary_river_chem = False\n",
    "river_TA, river_pH = [0, 0], [0]\n",
    "if 'freshwater_chemistry' in data:\n",
    "    vary_river_chem = True\n",
    "    river_TA = data['freshwater_chemistry']['river_TA']\n",
    "    river_pH = data['freshwater_chemistry']['river_pH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build root directories\n",
    "subprocess.call(['mkdir', 'editfiles'])\n",
    "subprocess.call(['mkdir', results_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize batchfile dict\n",
    "batchfile_dict = {\n",
    "    'max_concurrent_jobs': data['max_concurrent_jobs'],\n",
    "    'SOG_executable': os.path.join(data['paths']['SOG_code'], 'SOG'),\n",
    "    'base_infile': os.path.join(data['paths']['SOG_code'], 'infile.yaml'),\n",
    "    'jobs': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vary_forcing_parser(key, editfile_dict, forcing_inputs, run_year, run_tag):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Extra string for cloud fraction\n",
    "    string = ''\n",
    "    if key is 'river':\n",
    "        string = '_flows'\n",
    "    elif key is 'cloud':\n",
    "        string = '_fraction'\n",
    "        \n",
    "    # Vary forcing True or False\n",
    "    if forcing_inputs[0]:\n",
    "        editfile_dict[key + string] = {\n",
    "            'value': True,\n",
    "        }\n",
    "        run_tag = run_tag + '_{}'.format(key)\n",
    "\n",
    "        # Fixed\n",
    "        if forcing_inputs[1]:\n",
    "            editfile_dict[key + string + '_fixed'] = {\n",
    "                'value': True,\n",
    "            }\n",
    "            editfile_dict[key + string + '_value'] = {\n",
    "                'value': forcing_inputs[2],\n",
    "            }\n",
    "            run_tag = run_tag + '_fix{:.0f}'.format(forcing_inputs[2])\n",
    "\n",
    "        # Fraction\n",
    "        if forcing_inputs[3] != 1:\n",
    "            editfile_dict[key + string + '_fraction'] = {\n",
    "                'value': forcing_inputs[3],\n",
    "            }\n",
    "            run_tag = run_tag + '_frac{:.0f}'.format(forcing_inputs[3]*100)\n",
    "\n",
    "        # Add\n",
    "        if forcing_inputs[4] != 0:\n",
    "            editfile_dict[key + string + '_addition'] = {\n",
    "                'value': forcing_inputs[4],\n",
    "            }\n",
    "            run_tag = run_tag + '_add{:.0f}'.format(forcing_inputs[4])\n",
    "\n",
    "        # Shift\n",
    "        if forcing_inputs[5] != 0 or forcing_inputs[6] != 0:\n",
    "            run_tag = run_tag + '_shift'\n",
    "\n",
    "            # By year\n",
    "            if forcing_inputs[5] != 0:\n",
    "                editfile_dict[key + string + '_shift'] = {\n",
    "                    'value': forcing_inputs[5] - run_year,\n",
    "                }\n",
    "                run_tag = run_tag + '{:d}'.format(forcing_inputs[5])\n",
    "\n",
    "            # By day\n",
    "            if forcing_inputs[6] != 0:\n",
    "                editfile_dict[key + string + '_shift'] = {\n",
    "                    'value': editfile_dict[key + string + '_shift'] + forcing_inputs[6]/365,\n",
    "                }\n",
    "                run_tag = run_tag + '_{:.0f}'.format(forcing_inputs[6])\n",
    "\n",
    "    return editfile_dict, run_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_editfile_dict(\n",
    "    editfile_dict, datetimes, run_path, initial_path, ctd_dir, nuts_dir, init_files, hoffmueller=True\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse timestamps\n",
    "    datetime_start, datetime_end = map(parse, datetimes)\n",
    "    \n",
    "    editfile_dict.update({\n",
    "        'initial_conditions': {\n",
    "            'init_datetime': {\n",
    "                'value': datetime_start.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            },\n",
    "            'CTD_file': {\n",
    "                'value': os.path.join(initial_path, ctd_dir, init_files[0]),\n",
    "            },\n",
    "            'nutrients_file': {\n",
    "                'value': os.path.join(initial_path, nuts_dir, init_files[1]),\n",
    "            },\n",
    "        },\n",
    "        'end_datetime': {\n",
    "            'value': datetime_end.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        },\n",
    "        'timeseries_results': {\n",
    "            'std_physics': {\n",
    "                'value': os.path.join(run_path, 'timeseries/std_phys_SOG.out'),\n",
    "            },\n",
    "            'user_physics': {\n",
    "                'value': os.path.join(run_path, 'timeseries/user_phys_SOG.out'),\n",
    "            },\n",
    "            'std_biology': {\n",
    "                'value': os.path.join(run_path, 'timeseries/std_bio_SOG.out'),\n",
    "            },\n",
    "            'user_biology': {\n",
    "                'value': os.path.join(run_path, 'timeseries/user_bio_SOG.out'),\n",
    "            },\n",
    "            'std_chemistry': {\n",
    "                'value': os.path.join(run_path, 'timeseries/std_chem_SOG.out'),\n",
    "            },\n",
    "            'user_chemistry': {\n",
    "                'value': os.path.join(run_path, 'timeseries/user_chem_SOG.out'),\n",
    "            },\n",
    "        },\n",
    "        'profiles_results': {\n",
    "            'profile_file_base': {\n",
    "                'value': os.path.join(run_path, 'profiles/SOG'),\n",
    "            },\n",
    "            'user_profile_file_base': {\n",
    "                'value': os.path.join(run_path, 'profiles/SOG-user'),\n",
    "            },\n",
    "            'halocline_file': {\n",
    "                'value': os.path.join(run_path, 'profiles/halo-SOG.out'),\n",
    "            },\n",
    "            'hoffmueller_file': {\n",
    "                'value': os.path.join(run_path, 'profiles/hoff-SOG.dat'),\n",
    "            },\n",
    "            'user_hoffmueller_file': {\n",
    "                'value': os.path.join(run_path, 'profiles/hoff-SOG-user.dat'),\n",
    "            },\n",
    "            'hoffmueller_start_year': {\n",
    "                'value': datetime_start.year,\n",
    "            },\n",
    "            'hoffmueller_start_day': {\n",
    "                'value': (datetime_start - datetime(datetime_start.year, 1, 1)).days + 2,\n",
    "            },\n",
    "            'hoffmueller_end_year': {\n",
    "                'value': datetime_end.year,\n",
    "            },\n",
    "            'hoffmueller_end_day': {\n",
    "                'value': (datetime_end - datetime(datetime_end.year, 1, 1)).days,\n",
    "            },\n",
    "        },\n",
    "    })\n",
    "    \n",
    "    if not hoffmueller:\n",
    "        editfile_dict['profiles_results'].update({\n",
    "            'hoffmueller_end_year': {\n",
    "                'value': datetime_start.year,\n",
    "            },\n",
    "            'hoffmueller_end_day': {\n",
    "                'value': (datetime_start - datetime(datetime_start.year, 1, 1)).days + 3,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    return editfile_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001_TAvarpH74\n",
      "2001_TAvarpH77\n",
      "2001_TAvarpH80\n",
      "2001_TA500pH74\n",
      "2001_TA500pH77\n",
      "2001_TA500pH80\n",
      "2001_TA750pH74\n",
      "2001_TA750pH77\n",
      "2001_TA750pH80\n",
      "2001_TA1000pH74\n",
      "2001_TA1000pH77\n",
      "2001_TA1000pH80\n",
      "2002_TAvarpH74\n",
      "2002_TAvarpH77\n",
      "2002_TAvarpH80\n",
      "2002_TA500pH74\n",
      "2002_TA500pH77\n",
      "2002_TA500pH80\n",
      "2002_TA750pH74\n",
      "2002_TA750pH77\n",
      "2002_TA750pH80\n",
      "2002_TA1000pH74\n",
      "2002_TA1000pH77\n",
      "2002_TA1000pH80\n",
      "2003_TAvarpH74\n",
      "2003_TAvarpH77\n",
      "2003_TAvarpH80\n",
      "2003_TA500pH74\n",
      "2003_TA500pH77\n",
      "2003_TA500pH80\n",
      "2003_TA750pH74\n",
      "2003_TA750pH77\n",
      "2003_TA750pH80\n",
      "2003_TA1000pH74\n",
      "2003_TA1000pH77\n",
      "2003_TA1000pH80\n",
      "2004_TAvarpH74\n",
      "2004_TAvarpH77\n",
      "2004_TAvarpH80\n",
      "2004_TA500pH74\n",
      "2004_TA500pH77\n",
      "2004_TA500pH80\n",
      "2004_TA750pH74\n",
      "2004_TA750pH77\n",
      "2004_TA750pH80\n",
      "2004_TA1000pH74\n",
      "2004_TA1000pH77\n",
      "2004_TA1000pH80\n",
      "2005_TAvarpH74\n",
      "2005_TAvarpH77\n",
      "2005_TAvarpH80\n",
      "2005_TA500pH74\n",
      "2005_TA500pH77\n",
      "2005_TA500pH80\n",
      "2005_TA750pH74\n",
      "2005_TA750pH77\n",
      "2005_TA750pH80\n",
      "2005_TA1000pH74\n",
      "2005_TA1000pH77\n",
      "2005_TA1000pH80\n",
      "2006_TAvarpH74\n",
      "2006_TAvarpH77\n",
      "2006_TAvarpH80\n",
      "2006_TA500pH74\n",
      "2006_TA500pH77\n",
      "2006_TA500pH80\n",
      "2006_TA750pH74\n",
      "2006_TA750pH77\n",
      "2006_TA750pH80\n",
      "2006_TA1000pH74\n",
      "2006_TA1000pH77\n",
      "2006_TA1000pH80\n",
      "2007_TAvarpH74\n",
      "2007_TAvarpH77\n",
      "2007_TAvarpH80\n",
      "2007_TA500pH74\n",
      "2007_TA500pH77\n",
      "2007_TA500pH80\n",
      "2007_TA750pH74\n",
      "2007_TA750pH77\n",
      "2007_TA750pH80\n",
      "2007_TA1000pH74\n",
      "2007_TA1000pH77\n",
      "2007_TA1000pH80\n",
      "2008_TAvarpH74\n",
      "2008_TAvarpH77\n",
      "2008_TAvarpH80\n",
      "2008_TA500pH74\n",
      "2008_TA500pH77\n",
      "2008_TA500pH80\n",
      "2008_TA750pH74\n",
      "2008_TA750pH77\n",
      "2008_TA750pH80\n",
      "2008_TA1000pH74\n",
      "2008_TA1000pH77\n",
      "2008_TA1000pH80\n",
      "2009_TAvarpH74\n",
      "2009_TAvarpH77\n",
      "2009_TAvarpH80\n",
      "2009_TA500pH74\n",
      "2009_TA500pH77\n",
      "2009_TA500pH80\n",
      "2009_TA750pH74\n",
      "2009_TA750pH77\n",
      "2009_TA750pH80\n",
      "2009_TA1000pH74\n",
      "2009_TA1000pH77\n",
      "2009_TA1000pH80\n",
      "2010_TAvarpH74\n",
      "2010_TAvarpH77\n",
      "2010_TAvarpH80\n",
      "2010_TA500pH74\n",
      "2010_TA500pH77\n",
      "2010_TA500pH80\n",
      "2010_TA750pH74\n",
      "2010_TA750pH77\n",
      "2010_TA750pH80\n",
      "2010_TA1000pH74\n",
      "2010_TA1000pH77\n",
      "2010_TA1000pH80\n",
      "2011_TAvarpH74\n",
      "2011_TAvarpH77\n",
      "2011_TAvarpH80\n",
      "2011_TA500pH74\n",
      "2011_TA500pH77\n",
      "2011_TA500pH80\n",
      "2011_TA750pH74\n",
      "2011_TA750pH77\n",
      "2011_TA750pH80\n",
      "2011_TA1000pH74\n",
      "2011_TA1000pH77\n",
      "2011_TA1000pH80\n",
      "2012_TAvarpH74\n",
      "2012_TAvarpH77\n",
      "2012_TAvarpH80\n",
      "2012_TA500pH74\n",
      "2012_TA500pH77\n",
      "2012_TA500pH80\n",
      "2012_TA750pH74\n",
      "2012_TA750pH77\n",
      "2012_TA750pH80\n",
      "2012_TA1000pH74\n",
      "2012_TA1000pH77\n",
      "2012_TA1000pH80\n"
     ]
    }
   ],
   "source": [
    "# Loop through datetimes\n",
    "for datetimes, init_files in zip(\n",
    "    data['initialization']['datetimes'],\n",
    "    data['initialization']['files']\n",
    "):\n",
    "\n",
    "    # Parse run_year\n",
    "    run_year = parse(datetimes[0]).year + 1\n",
    "\n",
    "    # Build directories\n",
    "    subprocess.call(['mkdir', os.path.join('editfiles', str(run_year))])\n",
    "    subprocess.call(['mkdir', os.path.join(results_path, str(run_year))])\n",
    "\n",
    "    # Loop through forcing scenarios\n",
    "    for vary_forcing in vary_forcings:\n",
    "        \n",
    "        # Initialize editfile dict and run tag\n",
    "        editfile_dict = {}\n",
    "        run_tag = '{:d}'.format(run_year)\n",
    "        \n",
    "        # Parse forcing vary inputs\n",
    "        forcing_inputs = {'wind': vary_forcing[:7], 'river': vary_forcing[7:14], 'cloud': vary_forcing[14:]}\n",
    "        if any([forcing_inputs['wind'][0], forcing_inputs['river'][0], forcing_inputs['cloud'][0]]):\n",
    "            editfile_dict['vary'] = {}\n",
    "            for key in ['wind', 'river', 'cloud']:\n",
    "                editfile_dict['vary'], run_tag = vary_forcing_parser(\n",
    "                    key, editfile_dict['vary'], forcing_inputs[key], run_year, run_tag\n",
    "                )\n",
    "\n",
    "        # Loop through river chemistry scenarios\n",
    "        for TA in river_TA:\n",
    "            for pH in river_pH:\n",
    "\n",
    "                chem_tag = run_tag\n",
    "                \n",
    "                # Assign river chem params to editfile dict and update run_tag\n",
    "                # (if they exist)\n",
    "                if vary_river_chem:\n",
    "                    editfile_dict['physics'] = {\n",
    "                        'fresh_water': {'river_CO2_chemistry': {\n",
    "                            'river_TA_record': TA[0],\n",
    "                            'river_total_alkalinity': TA[1],\n",
    "                            'river_pH': pH,\n",
    "                        }}\n",
    "                    }\n",
    "                    if TA[0]:\n",
    "                        TA_val = 'var'\n",
    "                    else:\n",
    "                        TA_val = str(TA[1])\n",
    "                    chem_tag = chem_tag + '_TA{}pH{:.0f}'.format(TA_val, pH*10)\n",
    "\n",
    "                # Build paths and directories\n",
    "                editfile_path = os.path.join('editfiles', str(run_year), 'editfile_' + chem_tag)\n",
    "                run_path = os.path.join(results_path, str(run_year), chem_tag)\n",
    "                subprocess.call(['mkdir', run_path])\n",
    "\n",
    "                # Build editfile\n",
    "                editfile_dict = populate_editfile_dict(\n",
    "                    editfile_dict, datetimes, run_path, initial_path, ctd_dir, nuts_dir, init_files, hoffmueller=True,\n",
    "                )\n",
    "\n",
    "                # Append run list in batchfile\n",
    "                batchfile_dict['jobs'].append({chem_tag: {'edit_files': [editfile_path]}})\n",
    "\n",
    "                # Write editfile yaml\n",
    "                with open(editfile_path, 'w') as f:\n",
    "                    yaml.dump(editfile_dict, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('run_{}_{}'.format(data['machine'], data['batch_name']), 'w') as f:\n",
    "    yaml.dump(batchfile_dict, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
